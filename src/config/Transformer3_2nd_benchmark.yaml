# Archieved good results, with a Trapz > 90% and a train2test NN of 47% at peak and kmeans clustering > 90%
# No collaps to small numbers, thanks to higher weighting of lambda std (?)
data_spliting: "pos_test"
loss: "VicregLoss" 
model_name: "Transformer3"
batch_size: 6
class_hidden_size: 256
class_layers_nb: 1
conv_features1_nb: 32
conv_features2_nb: 30
conv_kernel1_size: 2
conv_kernel2_size: 2
conv_layers_nb: 3
data_limit: -1
additional_samples: 0 
same_positions: True
data_test_rate: 0.1
datafile: "/srv/public/Thibaud/datasets/ultrasec/Messung_10/messung10_9.2_data.npy"
device: "cuda"
dropout_value: 0
expender_out: 256
expender_layers_nb: 1
expender_hidden_size: 256
feature_norm: "layer"
flat_data: false
labelfile: "/srv/public/Thibaud/datasets/ultrasec/Messung_10/messung10_9.2_labels.npy"
latent_dimention: 200
learning_rate: 0.001
lr_limit: 0.0001
lambda_triplet: 1
lambda_class: 10
lambda_distance: 11    #14
lambda_std: 4        #1.2
lambda_cov: 4 
nb_epochs: 10000
noise_amount: 0
num_dev: 9
num_pos: 98
optimizer: "Adam"
padding_size: 0
patience: 250
plotting: false
set_parameters: "params.set_parameters"
sheduler: "plateau"
signal_length: 200
split_train_ratio: 0.8
stride_size: 1
tail_fc_layers_nb: 2
test_interval: 250
trans_embedding_size: 10
trans_head_nb: 1
trans_layer_nb: 4
trans_hidden_nb: 256
use_extender: false
use_gpu: true
use_wandb: true
validation_dev: 0
validation_pos: [5]
verbose: true
warmup_steps: 50
window_size: 16
data_type: "not complex"
data_use_position: false